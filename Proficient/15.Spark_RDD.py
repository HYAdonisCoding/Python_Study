#!/usr/bin/env python3
# coding: utf-8

import json
from operator import add
from pyspark import SparkConf, SparkContext, StorageLevel
import os

os.environ["SPARK_CONF_DIR"] = (
    "/Users/adam/Documents/Developer/MyGithub/Python_Study/Proficient"
)
# å…¨é¢é™éŸ³é…ç½®
os.environ["PYSPARK_SUBMIT_ARGS"] = (
    "--conf spark.ui.showConsoleProgress=false "
    "--conf spark.driver.extraJavaOptions='-Dorg.apache.commons.logging.Log=org.apache.commons.logging.impl.NoOpLog "
    "--add-opens=java.base/java.nio=ALL-UNNAMED "
    "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED "
    "--add-opens=java.base/java.util=ALL-UNNAMED "
    "--add-opens=java.base/java.lang=ALL-UNNAMED "
    "--add-opens=java.base/java.io=ALL-UNNAMED' "
    "pyspark-shell"
)
os.environ["HADOOP_OPTS"] = "-Djava.library.path="
os.environ["JAVA_TOOL_OPTIONS"] = "--add-opens=java.base/java.lang=ALL-UNNAMED"
speter = "-" * 10

# ç¬¬å››ç¯‡
# 15 Spark RDD ç¼–ç¨‹ Spark RDD Programming
# # å¯åŠ¨ç¯å¢ƒ
# unset PYSPARK_DRIVER_PYTHON
# unset PYSPARK_DRIVER_PYTHON_OPTS
# unset SPARK_CONNECT_MODE

# export PYSPARK_PYTHON=/Users/adam/envs/py_arm64/bin/python
# export PYSPARK_DRIVER_PYTHON=/Users/adam/envs/py_arm64/bin/python

# $SPARK_HOME/bin/pyspark
# $HADOOP_HOME/sbin/start-dfs.sh


def f1(item):
    return item, 1


def f2(item):
    return item > 1


def f3(item1, item2):
    return item1 + item2


# RDD ä¾èµ–å…³ç³»
def test():
    # 1ï¸âƒ£ åˆå§‹åŒ– SparkContext
    conf = SparkConf().setAppName("SparkRDDTest").setMaster("local[*]")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("ERROR")
    print("SparkContext Version", sc.version)
    # 2ï¸âƒ£ åˆ›å»º RDD
    data = [1, 2, 3, 4, 5, 6]
    rdd = sc.parallelize(data)

    # 3ï¸âƒ£ å®šä¹‰å‡½æ•°
    def f1(x):
        return x * 2

    def f2(x):
        return x > 5

    def f3(a, b):
        return a + b

    # 4ï¸âƒ£ RDD æ“ä½œ
    result = rdd.map(f1).filter(f2).reduce(f3)

    print(f"âœ… è®¡ç®—ç»“æœï¼š{result}")

    # 5ï¸âƒ£ åœæ­¢ SparkContext
    sc.stop()


# è¯»å–å¤–éƒ¨æ•°æ®æºåˆ›å»ºRDD
def read_outer_data():
    # 1ï¸âƒ£ åˆå§‹åŒ– SparkContext
    conf = SparkConf().setAppName("ReadOuterData").setMaster("local[*]")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("ERROR")

    try:
        # åˆ›å»ºä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
        test_file_path = "/tmp/spark_test.txt"
        with open(test_file_path, "w", encoding="utf-8") as f:
            f.write("Hello Spark\n")
            f.write("This is a test file\n")
            f.write("For Spark RDD operations\n")

        # è¯»å–æœ¬åœ°æ–‡ä»¶åˆ›å»ºRDD
        rdd = sc.textFile(f"file://{test_file_path}")

        # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹
        print("ğŸ“„ æ–‡ä»¶å†…å®¹:")
        lines = rdd.collect()
        for line in lines:
            print(f"  {line}")

        # ç»Ÿè®¡è¡Œæ•°
        line_count = rdd.count()
        print(f"ğŸ“Š æ–‡ä»¶æ€»è¡Œæ•°: {line_count}")

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        os.remove(test_file_path)

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        # åœæ­¢ SparkContext
        sc.stop()


# HDFSæ–‡ä»¶æ“ä½œç¤ºä¾‹
def hdfs_operations():
    """
    HDFSæ–‡ä»¶æ“ä½œç¤ºä¾‹
    æ³¨æ„ï¼šéœ€è¦å…ˆå¯åŠ¨Hadoopå’ŒHDFSæœåŠ¡
    """
    print("ğŸš€ å¼€å§‹HDFSæ“ä½œç¤ºä¾‹...")

    # 1ï¸âƒ£ åˆå§‹åŒ– SparkContext
    conf = SparkConf().setAppName("HDFSOperations").setMaster("local[*]")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("ERROR")

    try:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        local_file = "/tmp/hdfs_test.txt"
        hdfs_path = "/user/spark/test_file.txt"

        # å†™å…¥æµ‹è¯•æ•°æ®
        with open(local_file, "w", encoding="utf-8") as f:
            f.write("è¿™æ˜¯HDFSæµ‹è¯•æ–‡ä»¶\n")
            f.write("åŒ…å«ä¸­æ–‡å†…å®¹\n")
            f.write("ç”¨äºæ¼”ç¤ºSparkè¯»å–HDFSæ–‡ä»¶\n")
            f.write("æ•°æ®1,æ•°æ®2,æ•°æ®3\n")
            f.write("100,200,300\n")

        print(f"âœ… å·²åˆ›å»ºæœ¬åœ°æµ‹è¯•æ–‡ä»¶: {local_file}")

        # æ–¹æ³•1: ä½¿ç”¨Hadoopå‘½ä»¤ä¸Šä¼ æ–‡ä»¶åˆ°HDFS (éœ€è¦æ‰‹åŠ¨æ‰§è¡Œ)
        print("ğŸ“ æ‰‹åŠ¨ä¸Šä¼ æ–‡ä»¶åˆ°HDFSçš„å‘½ä»¤:")
        print(f"   hadoop fs -mkdir -p /user/spark")
        print(f"   hadoop fs -put {local_file} {hdfs_path}")
        print(f"   hadoop fs -ls /user/spark")

        # æ–¹æ³•2: ä½¿ç”¨Pythonçš„hdfs3åº“ä¸Šä¼ æ–‡ä»¶ (å¯é€‰)
        try:
            import hdfs3  # type: ignore

            hdfs = hdfs3.HDFileSystem()
            if hdfs.exists("/user/spark"):
                hdfs.rm("/user/spark", recursive=True)
            hdfs.makedirs("/user/spark")
            hdfs.put(local_file, hdfs_path)
            print(f"âœ… å·²ä½¿ç”¨hdfs3åº“ä¸Šä¼ æ–‡ä»¶åˆ°HDFS: {hdfs_path}")
        except ImportError:
            print("âš ï¸  hdfs3åº“æœªå®‰è£…ï¼Œè·³è¿‡è‡ªåŠ¨ä¸Šä¼ ")
        except Exception as e:
            print(f"âš ï¸  HDFSä¸Šä¼ å¤±è´¥: {e}")

        # è¯»å–HDFSæ–‡ä»¶
        print("ğŸ“– å°è¯•è¯»å–HDFSæ–‡ä»¶...")
        try:
            # è¯»å–HDFSæ–‡ä»¶
            rdd = sc.textFile(hdfs_path)

            # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹
            print("ğŸ“„ HDFSæ–‡ä»¶å†…å®¹:")
            lines = rdd.collect()
            for line in lines:
                print(f"  {line}")

            # ç»Ÿè®¡è¡Œæ•°
            line_count = rdd.count()
            print(f"ğŸ“Š HDFSæ–‡ä»¶æ€»è¡Œæ•°: {line_count}")

            # æ•°æ®å¤„ç†ç¤ºä¾‹
            print("ğŸ” æ•°æ®å¤„ç†ç¤ºä¾‹:")

            # è¿‡æ»¤åŒ…å«"æ•°æ®"çš„è¡Œ
            filtered_rdd = rdd.filter(lambda line: "æ•°æ®" in line)
            filtered_lines = filtered_rdd.collect()
            print(f"  åŒ…å«'æ•°æ®'çš„è¡Œ: {filtered_lines}")

            # ç»Ÿè®¡æ¯è¡Œçš„å­—ç¬¦æ•°
            char_count_rdd = rdd.map(lambda line: len(line))
            char_counts = char_count_rdd.collect()
            print(f"  æ¯è¡Œå­—ç¬¦æ•°: {char_counts}")

        except Exception as e:
            print(f"âŒ è¯»å–HDFSæ–‡ä»¶å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿:")
            print("   1. Hadoopå’ŒHDFSæœåŠ¡å·²å¯åŠ¨")
            print("   2. æ–‡ä»¶å·²ä¸Šä¼ åˆ°HDFS")
            print("   3. HDFSè·¯å¾„æ­£ç¡®")

        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        os.remove(local_file)

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        # åœæ­¢ SparkContext
        sc.stop()


# å®Œæ•´çš„HDFSæ“ä½œæµç¨‹ç¤ºä¾‹
def complete_hdfs_workflow():
    """
    å®Œæ•´çš„HDFSå·¥ä½œæµç¨‹ç¤ºä¾‹
    """
    print("ğŸ”„ å®Œæ•´HDFSå·¥ä½œæµç¨‹ç¤ºä¾‹...")

    # 1ï¸âƒ£ åˆå§‹åŒ– SparkContext
    conf = SparkConf().setAppName("CompleteHDFSWorkflow").setMaster("local[*]")
    sc = SparkContext(conf=conf)
    sc.setLogLevel("ERROR")

    try:
        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
        files_data = [
            ("file1.txt", ["ç”¨æˆ·1,è´­ä¹°,100", "ç”¨æˆ·2,è´­ä¹°,200", "ç”¨æˆ·3,é€€æ¬¾,50"]),
            ("file2.txt", ["ç”¨æˆ·4,è´­ä¹°,150", "ç”¨æˆ·5,è´­ä¹°,300", "ç”¨æˆ·6,é€€æ¬¾,75"]),
            ("file3.txt", ["ç”¨æˆ·7,è´­ä¹°,250", "ç”¨æˆ·8,è´­ä¹°,400", "ç”¨æˆ·9,é€€æ¬¾,100"]),
        ]

        hdfs_base_path = "/user/spark/sales_data"

        # åˆ›å»ºæœ¬åœ°æ–‡ä»¶
        local_files = []
        for filename, data in files_data:
            local_file = f"/tmp/{filename}"
            with open(local_file, "w", encoding="utf-8") as f:
                for line in data:
                    f.write(line + "\n")
            local_files.append((local_file, f"{hdfs_base_path}/{filename}"))
            print(f"âœ… åˆ›å»ºæœ¬åœ°æ–‡ä»¶: {local_file}")

        # æ˜¾ç¤ºä¸Šä¼ å‘½ä»¤
        print("\nğŸ“ ä¸Šä¼ æ–‡ä»¶åˆ°HDFSçš„å‘½ä»¤:")
        print(f"hadoop fs -mkdir -p {hdfs_base_path}")
        for local_file, hdfs_file in local_files:
            print(f"hadoop fs -put {local_file} {hdfs_file}")

        # è¯»å–HDFSç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        print(f"\nğŸ“– è¯»å–HDFSç›®å½•: {hdfs_base_path}")
        try:
            # è¯»å–æ•´ä¸ªç›®å½•
            rdd = sc.textFile(hdfs_base_path)

            print("ğŸ“„ æ‰€æœ‰æ–‡ä»¶å†…å®¹:")
            lines = rdd.collect()
            for line in lines:
                print(f"  {line}")

            # æ•°æ®åˆ†æç¤ºä¾‹
            print("\nğŸ” æ•°æ®åˆ†æ:")

            # è§£ææ•°æ®å¹¶è®¡ç®—æ€»é”€å”®é¢
            sales_rdd = rdd.filter(lambda line: "è´­ä¹°" in line)
            sales_amounts = sales_rdd.map(lambda line: int(line.split(",")[2]))
            total_sales = sales_amounts.sum()
            print(f"  æ€»é”€å”®é¢: {total_sales}")

            # ç»Ÿè®¡é€€æ¬¾é‡‘é¢
            refund_rdd = rdd.filter(lambda line: "é€€æ¬¾" in line)
            refund_amounts = refund_rdd.map(lambda line: int(line.split(",")[2]))
            total_refunds = refund_amounts.sum()
            print(f"  æ€»é€€æ¬¾é¢: {total_refunds}")

            # è®¡ç®—å‡€é”€å”®é¢
            net_sales = total_sales - total_refunds
            print(f"  å‡€é”€å”®é¢: {net_sales}")

        except Exception as e:
            print(f"âŒ è¯»å–HDFSç›®å½•å¤±è´¥: {e}")

        # æ¸…ç†æœ¬åœ°æ–‡ä»¶
        for local_file, _ in local_files:
            os.remove(local_file)

    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
    finally:
        sc.stop()


# ä½¿ç”¨æ•°ç»„åˆ›å»ºRDD
def array_RDD():
    sc = SparkContext()
    rdd = sc.parallelize([0, 1, 2, 3, 4, 6], 5)
    print(rdd.getNumPartitions())
    print(rdd.count())


# è½¬æ¢æ“ä½œ
def conversion_Operation():
    print(f"{speter*2}ä½¿ç”¨mapè½¬æ¢æ•°æ®{speter*2}")
    sc = SparkContext()
    rdd1 = sc.parallelize([0, 1, 2, 3, 4, 6])
    rdd2 = rdd1.map(lambda x: x * 2)
    local_data = rdd2.collect()
    [print("å½“å‰å…ƒç´ æ˜¯ï¼š", item) for item in local_data]

    print(f"{speter*2}ä½¿ç”¨ flatMap è½¬æ¢æ•°æ®{speter*2}")
    # sc = SparkContext()
    rdd1 = sc.parallelize(["lesson1 spark", "lesson2 hadoop", "lesson3 hive"])
    rdd2 = rdd1.flatMap(lambda x: x.split(" "))
    local_data = rdd2.collect()
    [print("å½“å‰å…ƒç´ æ˜¯ï¼š", item) for item in local_data]

    print(f"{speter*2}ä½¿ç”¨ filter è½¬æ¢æ•°æ®{speter*2}")
    rdd1 = sc.parallelize([0, 1, 2, 3, 4, 6])
    rdd2 = rdd1.filter(lambda x: x > 3)
    local_data = rdd2.collect()
    [print("å½“å‰å…ƒç´ æ˜¯ï¼š", item) for item in local_data]

    print(f"{speter*2}ä½¿ç”¨ groupByKey å°†æ•°æ®åˆ†ç»„{speter*2}")
    rdd1 = sc.parallelize([("a", 1), ("a", 1), ("a", 1), ("b", 1), ("b", 1), ("c", 1)])
    list1 = rdd1.groupByKey().mapValues(len).collect()
    [print("æŒ‰keyåˆ†ç»„åçš„æ•°æ®é¡¹ï¼š ", item) for item in list1]
    list2 = rdd1.groupByKey().mapValues(list).collect()
    [print("æ¯ä¸€ä¸ªkeyå¯¹åº”çš„æ•°æ®ï¼š", item) for item in list2]

    print(f"{speter*2}ä½¿ç”¨ reduceByKey å°†æ•°æ®åˆ†ç»„{speter*2}")
    rdd1 = sc.parallelize(["Spark", "Spark", "hadoop", "hadoop", "hadoop", "hive"])
    rdd2 = rdd1.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y).collect()
    [print("å½“å‰å…ƒç´ æ˜¯ï¼š ", item) for item in rdd2]

    print(f"{speter*2}ä½¿ç”¨ union åˆå¹¶RDD{speter*2}")
    rdd1 = sc.parallelize(["Spark", "hadoop", "hive"])
    rdd2 = sc.parallelize(["Spark", "kafka", "hbase"])
    rdd3 = rdd1.union(rdd2).collect()
    print("åˆå¹¶ç»“æœï¼š ", rdd3)

    print(f"{speter*2}ä½¿ç”¨ distinct å»é™¤é‡å¤æ•°æ®{speter*2}")
    rdd1 = sc.parallelize(["Spark", "hadoop", "hive"])
    rdd2 = sc.parallelize(["Spark", "kafka", "hbase"])
    rdd3 = rdd1.union(rdd2).distinct().collect()
    print("å»é™¤é‡å¤é¡¹ç»“æœï¼š ", rdd3)


# è¡ŒåŠ¨æ“ä½œ
def action_operation():
    print(f"{speter*2}ä½¿ç”¨ count è®¡ç®—æ•°æ®æ€»æ•°{speter*2}")
    sc = SparkContext()
    rdd = sc.parallelize(["Spark", "hadoop", "hive"])
    result = rdd.count()
    print("rddå…ƒç´ ä¸ªæ•°", result)

    print(f"{speter*2}ä½¿ç”¨ first è·å–ç¬¬ä¸€é¡¹{speter*2}")
    rdd = sc.parallelize([("a", 1), ("b", 2), ("c", 3), ("d", 4), ("e", 5)])
    result = rdd.sortBy(lambda x: x[1], False).first()
    print("å½“å‰å…ƒç´ æ˜¯ï¼š", result)

    print(f"{speter*2}ä½¿ç”¨ take è·å–å‰né¡¹{speter*2}")
    rdd = sc.parallelize([("a", 1), ("b", 2), ("c", 3), ("d", 4), ("e", 5)])
    result = rdd.sortBy(lambda x: x[1], False).take(3)
    print("å½“å‰å…ƒç´ æ˜¯ï¼š", result)

    print(f"{speter*2}ä½¿ç”¨ reduce {speter*2}")
    rdd = sc.parallelize([("a", 1), ("b", 2), ("c", 3), ("d", 4), ("e", 5)])

    result = rdd.map(lambda x: x[1]).reduce(add)
    print("å½“å‰ç»“æœï¼š", result)

    print(f"{speter*2}ä½¿ç”¨ foreach {speter*2}")
    rdd = sc.parallelize([("a", 1), ("b", 2), ("c", 3), ("d", 4), ("e", 5)], 2)

    def f(x):
        print("å½“å‰æ•°æ®é¡¹ï¼š", x)

    result = rdd.foreach(f)

    print(f"{speter*2}ä½¿ç”¨ foreachPartition {speter*2}")
    rdd = sc.parallelize([("a", 1), ("b", 2), ("c", 3), ("d", 4), ("e", 5)], 2)

    def f(iterator):
        print(list(iterator))

    result = rdd.foreachPartition(f)


# é”®å€¼å¯¹RDD
# # æŠŠæ–‡ä»¶ä» /user/adam/bigdata/chapter æ‹·è´åˆ° /bigdata/chapter
# hdfs dfs -cp /user/adam/bigdata/chapter/a_seafood.txt /bigdata/chapter/
def key_value_pairs_RDD():
    print(f"{speter*2}è¯»å–å¤–éƒ¨æ–‡ä»¶åˆ›å»ºPair RDD{speter*2}")
    sc = SparkContext()
    rdd1 = sc.textFile("/bigdata/chapter/a_seafood.txt")
    # rdd1 = sc.textFile("hdfs:///user/adam/bigdata/chapter/a_seafood.txt")

    def func(item):
        data = item.split(":")
        return data[0], data[1]

    rdd2 = rdd1.map(func)
    result = rdd2.collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]

    print(f"{speter*2}ä½¿ç”¨æ•°ç»„åˆ›å»ºPair RDD{speter*2}")
    # sc = SparkContext()
    rdd1 = sc.parallelize(["é»‘è™è™¾,æ‰‡è´,é»„èŠ±é±¼,é²ˆé±¼,ç½—éé±¼,é²œè´,é˜¿æ ¹å»·çº¢è™¾"])

    rdd2 = rdd1.flatMap(lambda item: item.split(",")).map(lambda item: (item, 1))
    result = rdd2.collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]

    print(f"{speter*2}å¸¸è§çš„é”®å€¼å¯¹è½¬æ¢æ“ä½œ{speter*2}")

    rdd1 = sc.parallelize(["é»‘è™è™¾,æ‰‡è´,é»„èŠ±é±¼,é²ˆé±¼,ç½—éé±¼,é²œè´,é˜¿æ ¹å»·çº¢è™¾"])
    rdd2 = rdd1.flatMap(lambda item: item.split(",")).map(lambda item: (item, 1))

    print("å½“å‰keyæ˜¯ï¼š", rdd2.keys().collect())
    print("å½“å‰valueæ˜¯ï¼š", rdd2.values().collect())

    print(f"{speter*2}ä½¿ç”¨ lookup è¿›è¡ŒæŸ¥æ‰¾{speter*2}")
    rdd1 = sc.textFile("/bigdata/chapter/a_seafood.txt")

    def func(item):
        data = item.split(":")
        return data[0], data[1]

    rdd2 = rdd1.map(func)
    result = rdd2.lookup("ç½—éé±¼")
    print("ç½—éé±¼ä»·æ ¼ï¼š", result)

    print(f"{speter*2}ä½¿ç”¨ zip ç»„åˆ RDD{speter*2}")
    rdd1 = sc.parallelize([139, 16.9, 49.9, 35.9, 29.9], 3)
    rdd2 = sc.parallelize(["é»‘è™è™¾", "æ‰‡è´", "é»„èŠ±é±¼", "é²ˆé±¼", "ç½—éé±¼"], 3)

    result = rdd2.zip(rdd1).collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]

    print(f"{speter*2}ä½¿ç”¨ join è¿æ¥ä¸¤ä¸ª RDD{speter*2}")
    rdd1 = sc.parallelize([("é»‘è™è™¾", 100), ("æ‰‡è´", 10.2), ("é²ˆé±¼", 59.9)])
    rdd2 = sc.parallelize(
        [("é»‘è™è™¾", 139), ("æ‰‡è´", 16.9), ("é²ˆé±¼", 35.9), ("ç½—éé±¼", 29.9)]
    )

    result = rdd1.join(rdd2).collect()
    print("joinç»“æœæ˜¯ï¼š", result)

    print(f"{speter*2}ä½¿ç”¨ leftOuterJoin è¿æ¥ä¸¤ä¸ª RDD{speter*2}")
    rdd1 = sc.parallelize([("é»‘è™è™¾", 100), ("æ‰‡è´", 10.2), ("æµ·å‚", 59.9)])
    rdd2 = sc.parallelize(
        [("é»‘è™è™¾", 139), ("æ‰‡è´", 16.9), ("é²ˆé±¼", 35.9), ("ç½—éé±¼", 29.9)]
    )

    result = rdd1.leftOuterJoin(rdd2).collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]
    print(f"{speter*2}ä½¿ç”¨ rightOuterJoin è¿æ¥ä¸¤ä¸ª RDD{speter*2}")
    result2 = rdd1.rightOuterJoin(rdd2).collect()

    [f(item) for item in result2]

    print(f"{speter*2}ä½¿ç”¨ fullOuterJoin è¿æ¥ä¸¤ä¸ª RDD{speter*2}")
    rdd1 = sc.parallelize([("é»‘è™è™¾", 100), ("æ‰‡è´", 10.2), ("æµ·å‚", 59.9)])
    rdd2 = sc.parallelize(
        [("é»‘è™è™¾", 139), ("æ‰‡è´", 16.9), ("é²ˆé±¼", 35.9), ("ç½—éé±¼", 29.9)]
    )

    result = rdd1.fullOuterJoin(rdd2).collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]

    print(f"{speter*2}ä½¿ç”¨ combineByKey{speter*2}")
    rdd = sc.parallelize(
        [
            ("é»‘è™è™¾", 139),
            ("é»‘è™è™¾", 100),
            ("æ‰‡è´", 16.9),
            ("æ‰‡è´", 10.2),
            ("æµ·å‚", 59.9),
            ("é²ˆé±¼", 35.9),
            ("ç½—éé±¼", 29.9),
        ]
    )

    def to_list(a):
        return [a]

    def append(a, b):
        a.append(b)
        return a

    def extend(a, b):
        a.extend(b)
        return a

    result = rdd.combineByKey(to_list, append, extend).collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]


# æ–‡ä»¶è¯»å†™
# hdfs dfs -mkdir -p /spark/files
# hdfs dfs -cp /Users/adam/Documents/Developer/MyGithub/Python_Study/README.md /spark/files/
# hdfs dfs -put /Users/adam/Documents/Developer/MyGithub/Python_Study/README.md /spark/files/
# éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸ hdfs dfs -ls /spark/files/
def file_reading_writing():
    print(f"{speter*2}è¯»å– HDFS å¹¶ä¿å­˜åˆ°æœ¬åœ°{speter*2}")
    sc = SparkContext()
    rdd = sc.textFile("/spark/files/README.md")
    # rdd.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).saveAsTextFile(
    #     "file:///tmp/filter_rdd_result/result.txt")

    print(f"{speter*2}è¯»å– HDFS å¹¶ä¿å­˜åˆ° HDFS{speter*2}")
    # rdd.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(lambda x, y: x + y).saveAsTextFile(
    # "/spark/files/filter_rdd/result.txt")

    print(f"{speter*2}è¯»å– æœ¬åœ°æ–‡ä»¶ å¹¶ä¿å­˜åˆ° HDFS{speter*2}")
    rdd = sc.textFile(
        "file:///Users/adam/Documents/Developer/MyGithub/Python_Study/README.md"
    )
    rdd.flatMap(lambda line: line.split(" ")).map(lambda word: (word, 1)).reduceByKey(
        lambda x, y: x + y
    ).saveAsTextFile("/spark/files/filter_rdd/result1.txt")


# ç¼–ç¨‹è¿›é˜¶
def advanced_Programming():
    print(f"{speter*2}åˆ†åŒº partitionBy{speter*2}")
    sc = SparkContext()

    pairs = sc.parallelize(
        [("é»‘è™è™¾", 139), ("æ‰‡è´", 16.9), ("é²ˆé±¼", 35.9), ("ç½—éé±¼", 29.9)]
    )
    sets = pairs.partitionBy(2).glom().collect()
    print(sets)

    print(f"{speter*2}è‡ªå®šä¹‰åˆ†åŒº{speter*2}")
    pairs = sc.parallelize(
        [
            ("é«˜å“è´¨", "é»‘è™è™¾"),
            ("ä¸€èˆ¬å“è´¨", "æ‰‡è´"),
            ("é«˜å“è´¨", "é²ˆé±¼"),
            ("ä¸€èˆ¬å“è´¨", "ç½—éé±¼"),
        ]
    )

    def custom_partition(key):
        if key == "é«˜å“è´¨":
            return 0
        else:
            return 1

    sets = pairs.partitionBy(2, partitionFunc=custom_partition).glom().collect()
    print(sets)

    print(f"{speter*2}é‡åˆ†åŒº coalesce{speter*2}")
    data = [
        ("é«˜å“è´¨", "é»‘è™è™¾"),
        ("ä¸€èˆ¬å“è´¨", "æ‰‡è´"),
        ("é«˜å“è´¨", "é²ˆé±¼"),
        ("ä¸€èˆ¬å“è´¨", "ç½—éé±¼"),
    ]
    sets1 = sc.parallelize(data, 4).glom().collect()
    print(sets1)
    sets2 = sc.parallelize(data, 4).coalesce(1).glom().collect()
    print(sets2)

    print(f"{speter*2}æŒä¹…åŒ– persist{speter*2}")
    data = [1, 2, 3, 4, 5, 6]

    def show(item):
        print("å½“å‰å…ƒç´ ", item)
        return item * 2

    rdd = sc.parallelize(data, 4).map(lambda x: show(x))
    rdd.persist(StorageLevel.MEMORY_ONLY)
    print("è·å–æœ€å°å€¼ï¼š", rdd.min())
    print("è·å–æœ€å¤§å€¼ï¼š", rdd.max())

    print(f"{speter*2}å¹¿æ’­å˜é‡{speter*2}")
    list1 = [2]
    broadcast = sc.broadcast(list1)

    list2 = [4, 5, 6]

    def f(item):
        broadcast_value = broadcast.value
        return item, broadcast_value[0]

    data = sc.parallelize(list2, 4).map(lambda x: f(x)).collect()

    [print("å½“å‰å…ƒç´ æ˜¯ï¼š", item) for item in data]

    print(f"{speter*2}ç´¯åŠ å™¨{speter*2}")
    list1 = [1, 2, 3, 4, 5, 6]

    accumulator = sc.accumulator(0)

    def f(item):
        accumulator.add(1)
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    data = sc.parallelize(list1, 4).foreach(lambda item: f(item))
    print("å¾ªç¯æ¬¡æ•°ï¼š", accumulator.value)

    print(f"{speter*2}json è½¬æ¢ä¸ºPair RDD{speter*2}")
    # package.json
    rdd = sc.textFile(
        "file:///Users/adam/Documents/Developer/MyGithub/Python_Study/Proficient/package_test.json"
    )

    def f(line):
        array = line.split(":")
        return array[0], array[1]

    # rdd.map(lambda line:f(line))
    def parse_json(line):
        try:
            data = json.loads(line)
            # éå† dependencies å­—æ®µ
            deps = data.get("dependencies", {})
            # è¿”å› (name, version) çš„åˆ—è¡¨
            # print(deps)
            return list(deps.items())
        except json.JSONDecodeError as e:
            print("JSONDecodeError", e)
            return []

    # flatMap å±•å¼€æ¯ä¸ªä¾èµ–
    pair_rdd = rdd.flatMap(parse_json)
    result = pair_rdd.collect()
    for item in result:
        print(item)

    print(f"{speter*2}checkpoint{speter*2}")

    sc.setCheckpointDir("/spark/checkpoint")
    rdd1 = sc.parallelize([1, 2, 3, 4, 5, 6])
    rdd2 = rdd1.map(lambda x: x * 2)
    rdd2.cache()
    rdd2.checkpoint()
    rdd2.sum()


# ç»Ÿè®¡æµ·é²œé”€å”®æƒ…å†µ
# hdfs dfs -put /Users/adam/Documents/Developer/MyGithub/Python_Study/Proficient/a_seal.txt /bigdata/chapter/
# hdfs dfs -put /Users/adam/Documents/Developer/MyGithub/Python_Study/Proficient/b_seal.txt /bigdata/chapter/
# éªŒè¯æ–‡ä»¶æ˜¯å¦ä¸Šä¼ æˆåŠŸ hdfs dfs -ls /bigdata/chapter/
def statistics_on_seafood_sales():
    print(f"{speter*2}ç»Ÿè®¡æµ·é²œé”€å”®æƒ…å†µ{speter*2}")
    sc = SparkContext()
    a_rdd = sc.textFile("/bigdata/chapter/a_seal.txt")
    b_rdd = sc.textFile("/bigdata/chapter/b_seal.txt")
    print(f"{speter*2}ç»Ÿè®¡å„å•†å“æ€»é”€é‡{speter*2}")
    union_rdd = a_rdd.union(b_rdd)

    def f(item):
        tmp = item.split(":")
        return tmp[0], int(tmp[1])

    map_rdd = union_rdd.map(f)
    result = map_rdd.reduceByKey(lambda x, y: x + y).collect()
    [print("å½“å‰å…ƒç´ æ˜¯ï¼š", item) for item in result]

    print(f"{speter*2}ç»Ÿè®¡å¹³å‡é”€é‡{speter*2}")

    map_rdd = union_rdd.map(f)
    map_rdd.cache()

    def create_combiner(v):
        return v, 1

    def merge_value(c, v):
        return c[0] + v, c[1] + 1

    def merge_combiners(c1, c2):
        return c1[0] + c2[0], c1[1] + c2[1]

    rdd = map_rdd.combineByKey(create_combiner, merge_value, merge_combiners)
    result = rdd.map(lambda x: (x[0], x[1][0] / x[1][1])).collect()

    def f(item):
        print("å½“å‰å…ƒç´ æ˜¯ï¼š", item)

    [f(item) for item in result]

    print(f"{speter*2}ç»Ÿè®¡é”€é‡æ’å{speter*2}")

    def f1(item):
        tmp = item.split(":")
        return tmp[0], int(tmp[1])

    a_map_rdd = a_rdd.map(f1)
    b_map_rdd = b_rdd.map(f1)
    join_rdd = a_map_rdd.join(b_map_rdd)

    def f2(item):
        return item[0], sum(item[1])

    result = join_rdd.map(f2).sortBy(lambda x: x[1], False).collect()
    [print(item) for item in result]


if __name__ == "__main__":
    print(f"{speter*2}Starting{speter*2}")
    try:
        statistics_on_seafood_sales()

    except KeyboardInterrupt:
        print(f"{speter*2}æ‰‹åŠ¨é€€å‡ºç¨‹åº{speter*2}")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {e}")
    finally:
        print(f"{speter*2}Finished{speter*2}")
