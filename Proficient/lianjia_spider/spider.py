#!/usr/bin/env python3
# filepath: /Users/adam/Documents/Developer/MyGithub/Python_Study/Proficient/recursion.py
# coding: utf-8
import json
import os
from storage import SpiderDB
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import random, time
import parser
from tqdm import tqdm
from selenium.common.exceptions import WebDriverException
import logging

# åœ¨æ–‡ä»¶é¡¶éƒ¨è®¾ç½®æ—¥å¿—ç­‰çº§
logging.getLogger("selenium").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


class HouseSpider:
    def __init__(self, headless=False):
        self.db = SpiderDB()
        self.setup_browser(False)
        if not self.load_cookies():
            self.login()

        print("ç™»å½•æµç¨‹å¤„ç†å®Œæˆã€‚")

    def setup_browser(self, headless=False):
        chrome_options = Options()
        if headless:
            chrome_options.add_argument("--headless")
        chrome_options.page_load_strategy = (
            "eager"  # åªç­‰ DOMContentLoadedï¼Œä¸ç­‰æ‰€æœ‰èµ„æº
        )
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-blink-features=AutomationControlled")
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option("useAutomationExtension", False)
        # chrome_options.add_argument("--blink-settings=imagesEnabled=false")

        # ç¦ç”¨å›¾ç‰‡åŠ è½½
        # prefs = {"profile.managed_default_content_settings.images": 2}
        # chrome_options.add_experimental_option("prefs", prefs)

        # ä¸‹è½½ä¸å½“å‰ Chrome ç‰ˆæœ¬å¯¹åº”çš„ ChromeDriver
        chrome_version = self.get_chrome_version()
        print(f"Detected Chrome version: {chrome_version}")
        # service = Service(ChromeDriverManager(driver_version=chrome_version).install())
        # service = Service(ChromeDriverManager().install())
        service = Service(
            ChromeDriverManager(driver_version="142.0.7444.135").install()
        )
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.execute_cdp_cmd("Network.enable", {})
        self.driver.execute_cdp_cmd(
            "Page.addScriptToEvaluateOnNewDocument",
            {
                "source": """
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                Object.defineProperty(navigator, 'platform', { get: () => 'MacIntel' });
                Object.defineProperty(navigator, 'userAgentData', { get: () => undefined });
            """
            },
        )
        chrome_options.add_experimental_option(
            "excludeSwitches", ["enable-automation", "enable-logging"]
        )
        chrome_options.add_experimental_option("useAutomationExtension", False)
        user_agent = (
            f"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 "
            f"(KHTML, like Gecko) Chrome/{chrome_version}.0.0.0 Safari/537.36"
        )
        chrome_options.add_argument(f"user-agent={user_agent}")

    def login(self):
        print("Opening Lianjia login page...")
        login_url = "https://bj.lianjia.com/"

        self.driver.get(login_url)

        print("\nè¯·ä½¿ç”¨ã€Œé“¾å®¶ App / è´å£³æ‰¾æˆ¿ Appã€æ‰«æäºŒç»´ç ç™»å½•â€¦")
        print("ç™»å½•æˆåŠŸåæŒ‰ Enter ç»§ç»­æ‰§è¡Œçˆ¬è™«ã€‚")

        input()  # ç”¨æˆ·ç¡®è®¤å·²ç™»å½•

        # è®°å½• Cookieï¼Œä¾¿äºä¸‹æ¬¡å…ç™»å½•
        cookies = self.driver.get_cookies()

        with open(
            os.path.join(BASE_DIR, "lianjia_cookies.json"), "w", encoding="utf-8"
        ) as f:
            json.dump(cookies, f)

        print("ç™»å½•æˆåŠŸï¼ŒCookies å·²ä¿å­˜ã€‚")

    def load_cookies(self):
        path = os.path.join(BASE_DIR, "lianjia_cookies.json")
        if not os.path.exists(path):
            print("No cookies found, need manual login.")
            return False

        self.driver.get("https://bj.lianjia.com/")  # å¿…é¡»å…ˆè®¿é—®åŸŸå
        with open(
            path, "r", encoding="utf-8"
        ) as f:
            cookies = json.load(f)

        for ck in cookies:
            ck.pop("expiry", None)  # é˜²æ­¢ Selenium æŠ¥é”™
            try:
                self.driver.add_cookie(ck)
            except Exception:
                continue

        print("Cookies loaded.")
        return True

    def get_chrome_version(self):
        # Helper method to get chrome version for driver manager
        import subprocess
        import re

        try:
            output = subprocess.check_output(["google-chrome", "--version"]).decode()
        except Exception:
            try:
                output = subprocess.check_output(
                    ["chromium-browser", "--version"]
                ).decode()
            except Exception:
                output = "Chrome 114.0.0.0"
        match = re.search(r"(\d+)\.", output)
        if match:
            return match.group(1)
        return None

    def human_scroll(
        self,
        min_step=30,
        max_step=80,
        base_delay=0.015,
        random_extra_delay=0.03,
        max_scroll_height=200,
    ):
        """
        ä¸æ»‘è¿ç»­çš„äººç±»æ»šåŠ¨æ–¹å¼ï¼ˆæ›´æ¥è¿‘çœŸå®ç”¨æˆ·é¼ æ ‡æ»šè½®ï¼‰

        :param max_scroll_height: æœ€å¤§æ»šåŠ¨é«˜åº¦é™åˆ¶ï¼Œæ»šåŠ¨åˆ°æ­¤é«˜åº¦æ—¶åœæ­¢
        """
        try:
            current_y = 0
            total_height = self.driver.execute_script(
                "return document.body.scrollHeight"
            )

            # å¦‚æœæ²¡æœ‰è®¾ç½®æœ€å¤§æ»šåŠ¨é«˜åº¦ï¼Œé»˜è®¤æ»šåŠ¨åˆ°åº•éƒ¨
            if max_scroll_height is None:
                max_scroll_height = total_height

            while current_y < max_scroll_height:
                # æ¯æ¬¡æ»šåŠ¨çš„åƒç´ ï¼ˆæ¨¡æ‹Ÿé¼ æ ‡æ»šè½®ï¼š30~80 pxï¼‰
                step = random.randint(min_step, max_step)
                current_y += step

                # ç¡®ä¿å½“å‰æ»šåŠ¨ä½ç½®ä¸è¶…è¿‡æœ€å¤§æ»šåŠ¨é™åˆ¶
                if current_y > max_scroll_height:
                    current_y = max_scroll_height

                self.driver.execute_script(f"window.scrollTo(0, {current_y});")

                # æ¨¡æ‹Ÿå¾®å°çš„äººç±»å»¶è¿Ÿï¼šåŸºç¡€ + éšæœº
                time.sleep(base_delay + random.random() * random_extra_delay)

                # åŠ¨æ€æ›´æ–°é¡µé¢é«˜åº¦ï¼ˆé¡µé¢æ‡’åŠ è½½æ—¶ï¼Œé¡µé¢å¯èƒ½ä¼šå¢åŠ é«˜åº¦ï¼‰
                total_height = self.driver.execute_script(
                    "return document.body.scrollHeight"
                )

                # å¦‚æœæ€»é«˜åº¦è¶…è¿‡æœ€å¤§æ»šåŠ¨é«˜åº¦ï¼Œåœæ­¢æ»šåŠ¨
                if total_height > max_scroll_height:
                    break

            # æ ¡å‡†æ»šåŠ¨ä½ç½®ï¼Œé¿å…å‡ºç°å¡é¡¿
            self.driver.execute_script(f"window.scrollTo(0, {current_y});")

        except Exception as e:
            print(f"Error during human scroll: {e}")

    def fetch_html(self, url, retries=2, wait_selector=None, wait_timeout=15):
        self.driver.execute_cdp_cmd(
            "Network.setExtraHTTPHeaders",
            {
                "headers": {
                    "Referer": "https://bj.lianjia.com/",
                    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                    "user-agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
                }
            },
        )
        for attempt in range(retries):
            try:
                self.driver.get(url)
                time.sleep(random.uniform(1, 3))  # ç®€å•ç­‰å¾… DOM åŠ è½½
                self.human_scroll()
                if wait_selector:
                    WebDriverWait(self.driver, wait_timeout).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, wait_selector))
                    )
                else:
                    time.sleep(random.uniform(3, 6))
                return self.driver.page_source
            except Exception as e:
                print(f"Error fetching {url}: {e}, retrying {attempt+1}/{retries}")

                return self.driver.page_source
        return None

    def parse_page(self, html):
        return parser.parse_list(html)

    def crawl_list_page(self, page=1, type=1):
        url = f"https://bj.lianjia.com/ershoufang/pg{page}/"
        html = self.fetch_html(url, wait_selector=".sellListContent")
        if html:
            data_list = self.parse_page(html)
            self.db.save_house_list(data_list)
            self.db.set_progress(page, type)
            return data_list
        else:
            print(f"Failed to crawl list page {page}")
            return None

    def crawl_detail_page(self, shop_url):
        html = self.fetch_html(shop_url, wait_selector=".baseinform")
        if html:
            # Assuming parser has a method to parse detail page
            detail_info = parser.parse_travel(html)
            # Save detail info if needed
            self.db.save_shop_detail(detail_info)
            current_detail_count = self.db.get_progress(key="detail_count")
            current_detail_count = (
                int(current_detail_count) if current_detail_count is not None else 0
            )
            self.db.set_progress(current_detail_count + 1, key="detail_count")
            return detail_info
        else:
            print(f"Failed to crawl detail page {shop_url}")
            return None

    def close(self):
        try:
            # æ‰“å°å½“å‰æ¨¡å¼
            mode = self.db.conn.execute("PRAGMA journal_mode;").fetchone()[0]
            print(f"Current SQLite journal mode: {mode}")

            # æ‰§è¡Œ checkpoint
            self.db.conn.execute("PRAGMA wal_checkpoint(TRUNCATE);")
            self.db.conn.commit()
            print("âœ… WAL checkpoint executed successfully.")

            # ç­‰å¾…ç³»ç»Ÿåˆ·æ–°æ–‡ä»¶é”
            self.db.conn.close()
            print("âœ… Database connection closed.")
            time.sleep(0.5)

            # äºŒæ¬¡éªŒè¯æ¸…ç†ï¼ˆä»…åœ¨æ‰€æœ‰è¿æ¥éƒ½æ–­å¼€çš„æƒ…å†µä¸‹ï¼‰
            if os.path.exists("dianping.db-wal"):
                os.remove("dianping.db-wal")
            if os.path.exists("dianping.db-shm"):
                os.remove("dianping.db-shm")
                print("ğŸ§¹ Residual SQLite cache files removed.")

        except Exception as e:
            print(f"âš ï¸ Error during database cleanup: {e}")


def get_list_data(type=1):
    dp_s = HouseSpider(headless=True)
    # è·å–ä¸Šæ¬¡è¿›åº¦ï¼Œæ”¯æŒæ–­ç‚¹ç»­æŠ“
    p = dp_s.db.get_progress(key="last_page")
    start_page = int(p or 1)
    print(p, start_page)
    if start_page < 1:
        start_page = 1

    end_page = 11
    for page in range(start_page, end_page + 1):
        print(f"Crawling page {page}...")
        try:
            data_list = dp_s.crawl_list_page(page, type)
            if not data_list:
                print(f"Page {page} failed, will retry later.")
                time.sleep(random.uniform(5, 10))  # ç»™è‡ªå·±å–˜å£æ°”
                continue
        except Exception as e:
            print(f"Unexpected error on page {page}: {e}, retrying...")
            time.sleep(random.uniform(5, 10))
            continue

        # éšæœºåœé¡¿ 3-8 ç§’ï¼Œé™ä½è¢«å°é£é™©
        time.sleep(random.uniform(3, 8))

    print("Crawling finished.")
    dp_s.close()

def fetch_missing_details():
    spider = HouseSpider(headless=False)   # å¯è§†åŒ–ç™»å½•æ›´ç¨³
    print("Checking missing house details...")

    BATCH_SIZE = 50

    while True:
        # ä½¿ç”¨ LEFT JOIN å¯»æ‰¾è¿˜æ²¡å…¥è¡¨çš„ house_idï¼Œé¿å¼€ NOT IN ä¸ NULL çš„å‘
        rows = spider.db.conn.execute(f"""
            SELECT h.house_id
            FROM houses h
            LEFT JOIN house_details d ON h.house_id = d.house_id
            WHERE d.house_id IS NULL
            LIMIT {BATCH_SIZE};
        """).fetchall()

        if not rows:
            print("ğŸ‰ æ‰€æœ‰æˆ¿æºè¯¦æƒ…å·²æŠ“å–å®Œæ¯•")
            break

        print(f"æœ¬è½®éœ€è¦å¤„ç† {len(rows)} æ¡æˆ¿æºè¯¦æƒ…")

        for idx, row in enumerate(rows, start=1):
            # row æ˜¯å•å…ƒç´  tupleï¼Œå–ç¬¬ 0 é¡¹
            house_id = row[0]
            print(f"[{idx}/{len(rows)}] Processing house_id = {house_id}")

            # æ„å»º detail urlï¼ˆæ³¨æ„ä¸è¦ä¼ å…¥ tupleï¼‰
            detail_url = f"https://bj.lianjia.com/ershoufang/{house_id}.html"

            # é‡è¯•æœºåˆ¶
            html = None
            for attempt in range(3):
                try:
                    html = spider.fetch_html(detail_url, wait_selector=".introContent", retries=2)
                    # è‹¥ fetch_html è¿”å›é¡µé¢ä½†è¢«é‡å®šå‘åˆ°ç™»å½•/é”™è¯¯é¡µï¼Œè¿›è¡ŒçŸ­å»¶æ—¶å†è¯•
                    if not html or "æŒ‡å®šçš„æœåŠ¡æœªè¢«æˆæƒ" in html or "clogin.lianjia.com" in html:
                        print(f"Attempt {attempt+1}: invalid page or login redirect for {house_id}")
                        time.sleep(random.uniform(2, 5))
                        continue
                    break
                except WebDriverException as we:
                    print(f"WebDriver error on {house_id}: {we}, retrying...")
                    time.sleep(random.uniform(2, 5))
                except Exception as e:
                    print(f"Unexpected error fetching {house_id}: {e}, retrying...")
                    time.sleep(random.uniform(1, 3))

            if not html:
                print(f"HTML ä¸ºç©ºæˆ–æŠ“å–å¤±è´¥ï¼Œè·³è¿‡ {house_id}")
                continue

            # è§£æè¯¦æƒ…é¡µï¼šè¯·ç¡®ä¿ parser.parse_detail å­˜åœ¨å¹¶è¿”å› dictï¼ˆåŒ…å« house_code æˆ– house_idï¼‰
            try:
                detail_info = parser.parse_detail(html)
                # å…¼å®¹ï¼šå¦‚æœè§£æå™¨è¿”å›æ²¡æœ‰ house_idï¼Œç”¨å½“å‰ house_id å¡«å……
                if not detail_info.get("house_id") and not detail_info.get("house_code"):
                    detail_info["house_code"] = house_id
            except Exception as e:
                print(f"è§£æå¤±è´¥ house_id={house_id}: {e}")
                # ä¿å­˜ HTML ä»¥ä¾¿æ‰‹å·¥åˆ†æ
                err_path = os.path.join(BASE_DIR, f"err_detail_{house_id}.html")
                with open(err_path, "w", encoding="utf-8") as f:
                    f.write(html)
                print(f"å·²ä¿å­˜é”™è¯¯é¡µé¢: {err_path}")
                continue

            # å…¥åº“
            try:
                spider.db.save_house_detail(detail_info)
                print(f"âœ“ Saved detail for {house_id}")
            except Exception as e:
                print(f"å…¥åº“å¤±è´¥ {house_id}: {e}")
                # å¦‚æœå› ä¸ºä¸»é”®å†²çªç­‰é—®é¢˜ï¼Œå¯ä»¥é€‰æ‹©æ›´æ–°æˆ–è®°å½•é”™è¯¯
                continue

            # æ¯æ¡ä¹‹é—´éšæœºåœé¡¿é™ä½è¢«å°é£é™©
            time.sleep(random.uniform(1.2, 3.5))

        # æ‰¹æ¬¡é—´ç­‰å¾…
        time.sleep(random.uniform(3, 6))

    spider.close()
    print("Details fetching completed.")


if __name__ == "__main__":
    print("Start testing DB schema and save logic")
    # 6 7 8 10 34 ->
    # get_list_data(type=-1)
    fetch_missing_details()
    print("Crawling finished.")
