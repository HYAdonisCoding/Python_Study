import os
import json
import sqlite3
from glob import glob
import csv
from datetime import datetime

# é…ç½®è·¯å¾„
BASE_DIR = "./data"
DB_FILE = os.path.join(BASE_DIR, "pesticide_data.db")
PROGRESS_FILE = os.path.join(BASE_DIR, "import_progress.txt")
LOG_FILE = os.path.join(BASE_DIR, 'db_import_log.txt')

def load_progress():
    if not os.path.exists(PROGRESS_FILE):
        return 1
    with open(PROGRESS_FILE, "r") as f:
        return int(f.read().strip() or 1)

def save_progress(page_no):
    with open(PROGRESS_FILE, "w") as f:
        f.write(str(page_no))
        
def log_entry(message):
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open(LOG_FILE, 'a', encoding='utf-8') as f:
        f.write(f"[{timestamp}] {message}\n")
def data_todb(end_page=None):
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS pesticide_data (
        ç™»è®°è¯å· TEXT PRIMARY KEY,
        å†œè¯åç§° TEXT,
        å†œè¯ç±»åˆ« TEXT,
        å‰‚å‹ TEXT,
        æ€»å«é‡ TEXT,
        æœ‰æ•ˆæœŸè‡³ TEXT,
        ç™»è®°è¯æŒæœ‰äºº TEXT,
        pd_id TEXT,
        ç™»è®°è¯ä¿¡æ¯ TEXT,
        æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ TEXT,
        åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯ TEXT
    )
    """)
    conn.commit()

    start_page = load_progress()
    json_files = sorted(glob(os.path.join(BASE_DIR, "page_*.json")))
    total_inserted = 0
    total_updated = 0

    for file_path in json_files:
        filename = os.path.basename(file_path)
        try:
            page_no = int(filename.split('_')[1].split('.')[0])
        except ValueError:
            continue

        if page_no < start_page:
            continue
        # é¡µç è¿‡æ»¤
        if page_no < start_page:
            continue
        if end_page is not None and page_no > end_page:
            break
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                records = json.load(f)

            insert_count = 0
            update_count = 0

            for entry in records:
                djzh = entry.get("ç™»è®°è¯å·")
                if not djzh:
                    continue

                cursor.execute("SELECT 1 FROM pesticide_data WHERE ç™»è®°è¯å· = ?", (djzh,))
                exists = cursor.fetchone() is not None

                cursor.execute("""
                    INSERT INTO pesticide_data (
                        ç™»è®°è¯å·, å†œè¯åç§°, å†œè¯ç±»åˆ«, å‰‚å‹, æ€»å«é‡, æœ‰æ•ˆæœŸè‡³,
                        ç™»è®°è¯æŒæœ‰äºº, pd_id, ç™»è®°è¯ä¿¡æ¯, æœ‰æ•ˆæˆåˆ†ä¿¡æ¯, åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ON CONFLICT(ç™»è®°è¯å·) DO UPDATE SET
                        å†œè¯åç§°=excluded.å†œè¯åç§°,
                        å†œè¯ç±»åˆ«=excluded.å†œè¯ç±»åˆ«,
                        å‰‚å‹=excluded.å‰‚å‹,
                        æ€»å«é‡=excluded.æ€»å«é‡,
                        æœ‰æ•ˆæœŸè‡³=excluded.æœ‰æ•ˆæœŸè‡³,
                        ç™»è®°è¯æŒæœ‰äºº=excluded.ç™»è®°è¯æŒæœ‰äºº,
                        pd_id=excluded.pd_id,
                        ç™»è®°è¯ä¿¡æ¯=excluded.ç™»è®°è¯ä¿¡æ¯,
                        æœ‰æ•ˆæˆåˆ†ä¿¡æ¯=excluded.æœ‰æ•ˆæˆåˆ†ä¿¡æ¯,
                        åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯=excluded.åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯
                """, (
                    djzh,
                    entry.get("å†œè¯åç§°"),
                    entry.get("å†œè¯ç±»åˆ«"),
                    entry.get("å‰‚å‹"),
                    entry.get("æ€»å«é‡"),
                    entry.get("æœ‰æ•ˆæœŸè‡³"),
                    entry.get("ç™»è®°è¯æŒæœ‰äºº"),
                    entry.get("pd_id"),
                    json.dumps(entry.get("ç™»è®°è¯ä¿¡æ¯", ""), ensure_ascii=False),
                    json.dumps(entry.get("æœ‰æ•ˆæˆåˆ†ä¿¡æ¯", []), ensure_ascii=False),
                    json.dumps(entry.get("åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯", []), ensure_ascii=False),
                ))

                if exists:
                    update_count += 1
                else:
                    insert_count += 1

            conn.commit()
            total_inserted += insert_count
            total_updated += update_count
            save_progress(page_no + 1)

            log_entry(f"âœ… {filename} å¯¼å…¥å®Œæˆï¼šæ–°å¢ {insert_count} æ¡ï¼Œæ›´æ–° {update_count} æ¡")
            print(f"âœ… å·²å¯¼å…¥ {filename}ï¼Œæ–°å¢ {insert_count}ï¼Œæ›´æ–° {update_count}")

        except Exception as e:
            log_entry(f"âŒ {filename} å¯¼å…¥å¤±è´¥ï¼š{e}")
            print(f"âŒ é”™è¯¯ï¼š{filename} å¯¼å…¥å¤±è´¥ - {e}")

    conn.close()
    log_entry(f"âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±æ–°å¢ {total_inserted} æ¡ï¼Œæ›´æ–° {total_updated} æ¡")
    print(f"âœ… æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±æ–°å¢ {total_inserted} æ¡ï¼Œæ›´æ–° {total_updated} æ¡")


CSV_FILE = os.path.join(BASE_DIR, "pesticide_data_export.csv")
# å¯¼å‡ºæ•°æ®åˆ° CSV
def export_db_to_csv(db_path, csv_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # è·å–æ‰€æœ‰æ•°æ®
    cursor.execute("SELECT * FROM pesticide_data")
    rows = cursor.fetchall()

    # è·å–åˆ—å
    column_names = [description[0] for description in cursor.description]

    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(column_names)  # å†™å…¥è¡¨å¤´
        for row in rows:
            # æŠŠ json åˆ—è½¬ä¸ºçº¯æ–‡æœ¬ï¼ˆå¦‚æœ‰ï¼‰
            processed_row = []
            for value in row:
                if isinstance(value, str):
                    try:
                        val = json.loads(value)
                        if isinstance(val, (dict, list)):
                            processed_row.append(json.dumps(val, ensure_ascii=False))
                        else:
                            processed_row.append(val)
                    except:
                        processed_row.append(value)
                else:
                    processed_row.append(value)
            writer.writerow(processed_row)

    conn.close()
    print(f"âœ… æ•°æ®å·²æˆåŠŸå¯¼å‡ºè‡³ {csv_path}")

def export_db_to_csv_pro(db_path, csv_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # è·å–æ‰€æœ‰æ•°æ®
    cursor.execute("SELECT * FROM pesticide_data")
    rows = cursor.fetchall()

    # è·å–åˆ—å
    column_names = [description[0] for description in cursor.description]

    # å…ˆæ‰¾å‡ºæœ€å¤§æœ‰æ•ˆæˆåˆ†æ•°é‡
    max_components = 0
    comp_lists = []
    for row in rows:
        val = row[column_names.index("æœ‰æ•ˆæˆåˆ†ä¿¡æ¯")]
        try:
            comps = json.loads(val) if val else []
            if not isinstance(comps, list):
                comps = []
        except:
            comps = []
        comp_lists.append(comps)
        max_components = max(max_components, len(comps))

    # ç”Ÿæˆæ–°çš„åˆ—åï¼Œè¿½åŠ æœ‰æ•ˆæˆåˆ†åˆ—
    expanded_column_names = column_names.copy()
    for i in range(1, max_components + 1):
        expanded_column_names.extend([
            f"æœ‰æ•ˆæˆåˆ†{i}",
            f"æœ‰æ•ˆæˆåˆ†{i}è‹±æ–‡",
            f"æœ‰æ•ˆæˆåˆ†{i}å«é‡"
        ])

    # å†™å…¥ CSV
    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(expanded_column_names)

        for row, comps in zip(rows, comp_lists):
            processed_row = list(row)  # å¤åˆ¶åŸå§‹æ•°æ®
            # è¿½åŠ æ¯ä¸ªæœ‰æ•ˆæˆåˆ†çš„å€¼
            for comp in comps:
                processed_row.append(comp.get("æœ‰æ•ˆæˆåˆ†", ""))
                processed_row.append(comp.get("æœ‰æ•ˆæˆåˆ†è‹±æ–‡å", ""))
                processed_row.append(comp.get("æœ‰æ•ˆæˆåˆ†å«é‡", ""))
            # å¦‚æœè¿™ä¸€è¡Œæœ‰æ•ˆæˆåˆ†å°‘äº max_componentsï¼Œè¡¥ç©ºåˆ—
            empty_cols = (max_components - len(comps)) * 3
            processed_row.extend([""] * empty_cols)

            writer.writerow(processed_row)

    conn.close()
    print(f"âœ… æ•°æ®å·²æˆåŠŸå¯¼å‡ºè‡³ {csv_path}")
def export_db_to_csv_pro_max(db_path, csv_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("SELECT * FROM pesticide_data")
    rows = cursor.fetchall()
    column_names = [desc[0] for desc in cursor.description]

    idx_reg_info = column_names.index("ç™»è®°è¯ä¿¡æ¯")
    idx_comp_info = column_names.index("æœ‰æ•ˆæˆåˆ†ä¿¡æ¯")
    idx_dosage_info = column_names.index("åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯")

    # ===== è§£æç™»è®°è¯ä¿¡æ¯ï¼ˆå»é‡å­—æ®µï¼‰=====
    main_fields = set(column_names)
    reginfo_keys = set()
    reginfo_list = []

    for row in rows:
        try:
            info = json.loads(row[idx_reg_info]) if row[idx_reg_info] else {}
            if not isinstance(info, dict):
                info = {}
        except:
            info = {}
        reginfo_list.append(info)
        for k in info.keys():
            if k not in main_fields:
                reginfo_keys.add(k)

    reginfo_keys = sorted(reginfo_keys)

    # ===== è§£ææœ‰æ•ˆæˆåˆ†ä¿¡æ¯ =====
    comp_lists = []
    max_components = 0
    for row in rows:
        try:
            comps = json.loads(row[idx_comp_info]) if row[idx_comp_info] else []
            if not isinstance(comps, list):
                comps = []
        except:
            comps = []
        comp_lists.append(comps)
        max_components = max(max_components, len(comps))

    # ===== è§£æåˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯ =====
    dosage_lists = []
    max_dosages = 0
    for row in rows:
        try:
            dosages = json.loads(row[idx_dosage_info]) if row[idx_dosage_info] else []
            if not isinstance(dosages, list):
                dosages = []
        except:
            dosages = []
        dosage_lists.append(dosages)
        max_dosages = max(max_dosages, len(dosages))

    # ===== æ„é€ è¡¨å¤´ =====
    base_columns = [
        c for c in column_names
        if c not in ("ç™»è®°è¯ä¿¡æ¯", "æœ‰æ•ˆæˆåˆ†ä¿¡æ¯", "åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯")
    ]

    expanded_columns = base_columns + reginfo_keys

    for i in range(1, max_components + 1):
        expanded_columns.extend([
            f"æœ‰æ•ˆæˆåˆ†{i}",
            f"æœ‰æ•ˆæˆåˆ†{i}è‹±æ–‡",
            f"æœ‰æ•ˆæˆåˆ†{i}å«é‡"
        ])

    for i in range(1, max_dosages + 1):
        expanded_columns.extend([
            f"ç”¨è¯{i}_ä½œç‰©/åœºæ‰€",
            f"ç”¨è¯{i}_é˜²æ²»å¯¹è±¡",
            f"ç”¨è¯{i}_ç”¨è¯é‡",
            f"ç”¨è¯{i}_æ–½ç”¨æ–¹æ³•"
        ])

    # ===== å†™å…¥ CSV =====
    with open(csv_path, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(expanded_columns)

        for row, reginfo, comps, dosages in zip(rows, reginfo_list, comp_lists, dosage_lists):
            base_row = [
                row[column_names.index(c)] for c in base_columns
            ]

            # ç™»è®°è¯ä¿¡æ¯è¡¥å……å­—æ®µ
            for k in reginfo_keys:
                base_row.append(reginfo.get(k, ""))

            # æœ‰æ•ˆæˆåˆ†å±•å¼€
            for comp in comps:
                base_row.append(comp.get("æœ‰æ•ˆæˆåˆ†", ""))
                base_row.append(comp.get("æœ‰æ•ˆæˆåˆ†è‹±æ–‡å", ""))
                base_row.append(comp.get("æœ‰æ•ˆæˆåˆ†å«é‡", ""))
            base_row.extend([""] * ((max_components - len(comps)) * 3))

            # åˆ¶å‰‚ç”¨è¯é‡å±•å¼€
            for d in dosages:
                base_row.append(d.get("ä½œç‰©/åœºæ‰€", ""))
                base_row.append(d.get("é˜²æ²»å¯¹è±¡", ""))
                base_row.append(d.get("ç”¨è¯é‡", ""))
                base_row.append(d.get("æ–½ç”¨æ–¹æ³•", ""))
            base_row.extend([""] * ((max_dosages - len(dosages)) * 4))

            writer.writerow(base_row)

    conn.close()
    print(f"âœ… æ•°æ®å·²æˆåŠŸå¯¼å‡ºè‡³ {csv_path}")
# ç»Ÿè®¡æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ä¸­ æœ‰æ•ˆæˆåˆ†ã€æœ‰æ•ˆæˆåˆ†è‹±æ–‡å å‡ºç°çš„æ¬¡æ•°
def statistical_information_effective_components(db_path=DB_FILE, top=10):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("SELECT æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ FROM pesticide_data")
    rows = cursor.fetchall()

    from collections import Counter
    cn_counter = Counter()
    en_counter = Counter()

    for (val,) in rows:
        try:
            comps = json.loads(val) if val else []
            if not isinstance(comps, list):
                continue
        except:
            continue

        for comp in comps:
            cn = comp.get("æœ‰æ•ˆæˆåˆ†")
            en = comp.get("æœ‰æ•ˆæˆåˆ†è‹±æ–‡å")
            if cn:
                cn_counter[cn] += 1
            if en:
                en_counter[en] += 1

    conn.close()

    # ===== ç»ˆç«¯è¾“å‡º =====
    print(f"\nğŸ“Š æœ‰æ•ˆæˆåˆ†ï¼ˆä¸­æ–‡ï¼‰å‡ºç°æ¬¡æ•° Top {top}:")
    for name, cnt in cn_counter.most_common(top):
        print(f"{name}ï¼š{cnt}")

    print(f"\nğŸ“Š æœ‰æ•ˆæˆåˆ†ï¼ˆè‹±æ–‡ï¼‰å‡ºç°æ¬¡æ•° Top {top}:")
    for name, cnt in en_counter.most_common(top):
        print(f"{name}ï¼š{cnt}")

    print(f"\nâœ… ç»Ÿè®¡å®Œæˆï¼šå…±ç»Ÿè®¡ä¸­æ–‡ {len(cn_counter)} ç§ï¼Œè‹±æ–‡ {len(en_counter)} ç§æœ‰æ•ˆæˆåˆ†")

    # ===== å†™å…¥ CSV =====
    cn_csv = os.path.join(BASE_DIR, "effective_components_cn_stats.csv")
    en_csv = os.path.join(BASE_DIR, "effective_components_en_stats.csv")

    with open(cn_csv, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["æœ‰æ•ˆæˆåˆ†", "å‡ºç°æ¬¡æ•°"])
        for name, cnt in cn_counter.most_common():
            writer.writerow([name, cnt])

    with open(en_csv, "w", encoding="utf-8-sig", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["æœ‰æ•ˆæˆåˆ†è‹±æ–‡å", "å‡ºç°æ¬¡æ•°"])
        for name, cnt in en_counter.most_common():
            writer.writerow([name, cnt])

    print(f"\nğŸ“ CSV å·²ç”Ÿæˆï¼š")
    print(f" - {cn_csv}")
    print(f" - {en_csv}")
    
    
# æ‰§è¡Œå¯¼å‡º
if __name__ == '__main__':
    # data_todb()
    statistical_information_effective_components(DB_FILE)
