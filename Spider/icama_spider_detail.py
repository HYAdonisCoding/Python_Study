import os
import json
import random
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from tqdm import tqdm

DATA_DIR = "data"

def build_url(pd_id):
    r_value = round(random.random(), 16)
    return f"https://www.icama.cn/BasicdataSystem/pesticideRegistration/viewpd.do?r={r_value}&id={pd_id}"

def create_driver():
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    return webdriver.Chrome(options=options)

def parse_detail_page(soup):
    tables = soup.find_all('table', {'id': 'reg'})
    result = {
        "ç™»è®°è¯ä¿¡æ¯": {},
        "æœ‰æ•ˆæˆåˆ†ä¿¡æ¯": [],
        "åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯": []
    }

    if len(tables) >= 1:
        for row in tables[0].find_all('tr')[1:]:
            cols = [td.get_text(strip=True) for td in row.find_all('td')]
            for i in range(0, len(cols), 2):
                key = cols[i].replace("ï¼š", "").strip()
                value = cols[i + 1] if i + 1 < len(cols) else ""
                if key:
                    result["ç™»è®°è¯ä¿¡æ¯"][key] = value

    if len(tables) >= 3:
        for row in tables[2].find_all('tr')[2:]:
            cols = [td.get_text(strip=True) for td in row.find_all('td')]
            if len(cols) == 3:
                result["æœ‰æ•ˆæˆåˆ†ä¿¡æ¯"].append({
                    "æœ‰æ•ˆæˆåˆ†": cols[0],
                    "æœ‰æ•ˆæˆåˆ†è‹±æ–‡å": cols[1],
                    "æœ‰æ•ˆæˆåˆ†å«é‡": cols[2]
                })

    if len(tables) >= 4:
        for row in tables[3].find_all('tr')[2:]:
            cols = [td.get_text(strip=True) for td in row.find_all('td')]
            if len(cols) >= 4:
                result["åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯"].append({
                    "ä½œç‰©/åœºæ‰€": cols[0],
                    "é˜²æ²»å¯¹è±¡": cols[1],
                    "ç”¨è¯é‡": cols[2],
                    "æ–½ç”¨æ–¹æ³•": cols[3]
                })

    return result

def get_detail_from_file(driver, file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    for i, item in tqdm(enumerate(data), total=len(data), desc=f"ğŸ“„ å¤„ç† {os.path.basename(file_path)}"):
        pd_id = item.get("pd_id")
        if not pd_id:
            continue

        url = build_url(pd_id)
        # print(f"[{i + 1}/{len(data)}] è¯·æ±‚è¯¦æƒ…é¡µï¼š{url}")
        try:
            driver.get(url)
            time.sleep(random.uniform(1.5, 3.0))  # å¯è°ƒèŠ‚
            soup = BeautifulSoup(driver.page_source, "html.parser")
            detail_info = parse_detail_page(soup)
            item.update(detail_info)
        except Exception as e:
            print(f"âŒ æŠ“å–å¤±è´¥ï¼š{e}")
    return data

def save_json(data, output_file):
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def extract_batch(start=1, end=5):
    driver = create_driver()
    try:
        for page_num in range(start, end + 1):
            file_name = f"page_{page_num:04d}.json"
            file_path = os.path.join(DATA_DIR, file_name)
            if not os.path.exists(file_path):
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_name}")
                continue

            print(f"\n--- æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_name} ---")
            updated_data = get_detail_from_file(driver, file_path)
            save_json(updated_data, file_path)
            print(f"âœ… ä¿å­˜å®Œæˆ: {file_name}")
    finally:
        driver.quit()
        print("âœ… æ‰€æœ‰è¯¦æƒ…é¡µå¤„ç†å®Œæˆ")

if __name__ == "__main__":
    print(f"å¼€å§‹å¤„ç†è¯¦æƒ…é¡µ{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}")
    extract_batch(start=1382, end=1533)