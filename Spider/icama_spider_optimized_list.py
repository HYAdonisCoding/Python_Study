from datetime import datetime
import os
import re
import time
import json
import sqlite3
import logging
import random
import requests, certifi
from bs4 import BeautifulSoup
from tqdm import tqdm

# === é…ç½®åŒº ===
DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "pesticide_data.db")
PROGRESS_FILE = os.path.join(DATA_DIR, "sync_progress.txt")
LOG_FILE = os.path.join(DATA_DIR, "sync_log.txt")

os.makedirs(DATA_DIR, exist_ok=True)

# === æ—¥å¿—é…ç½® ===
logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
    "Accept-Language": "zh-CN,zh;q=0.9",
    "Connection": "keep-alive",
    "DNT": "1",
    "Referer": "http://www.icama.org.cn/",
    "Sec-Fetch-Dest": "iframe",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "cross-site",
    "Sec-Fetch-Storage-Access": "none",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36",
    "sec-ch-ua": '"Google Chrome";v="137", "Chromium";v="137", "Not/A)Brand";v="24"',
    "sec-ch-ua-mobile": "?0",
    "sec-ch-ua-platform": '"macOS"',
}


# === å·¥å…·å‡½æ•° ===
def save_progress(page):
    with open(PROGRESS_FILE, "w") as f:
        f.write(str(page))


def load_progress():
    if os.path.exists(PROGRESS_FILE):
        with open(PROGRESS_FILE) as f:
            return int(f.read().strip() or 1)
    return 1


def fetch_list_page(page):
    url = "https://www.icama.cn/BasicdataSystem/pesticideRegistration/queryselect.do"
    data = {
        "pageNo": page,
        "pageSize": 20,
        "djzh": "",
        "nymc": "",
        "cjmc": "",
        "sf": "",
        "nylb": "",
        "zhl": "",
        "jx": "",
        "zwmc": "",
        "fzdx": "",
        "syff": "",
        "dx": "",
        "yxcf": "",
        "yxcf_en": "",
        "yxcfhl": "",
        "yxcf2": "",
        "yxcf2_en": "",
        "yxcf2hl": "",
        "yxcf3": "",
        "yxcf3_en": "",
        "yxcf3hl": "",
        "yxqs_start": "",
        "yxqs_end": "",
        "yxjz_start": "",
        "yxjz_end": "",
        "accOrfuzzy": "2",
    }
    try:
        response = requests.post(url, headers=HEADERS, verify=False, data=data, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e_post:
        logging.warning(f"âš ï¸ POST è¯·æ±‚å¤±è´¥ï¼Œç¬¬ {page} é¡µ: {e_post}")

        # å°è¯• GET å¤‡é€‰æ–¹æ¡ˆ
        try:
            response = requests.get(url, headers=HEADERS, verify=False, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e_get:
            logging.error(f"âŒ GET å¤‡é€‰æ–¹æ¡ˆå¤±è´¥ï¼Œç¬¬ {page} é¡µ: {e_get}")
            return None


def parse_list(html):
    soup = BeautifulSoup(html, "html.parser")
    viewpd_pattern = re.compile(r"_viewpd\('([^']+)'\)")
    table = soup.find_all("table")[1]
    rows = table.find_all("tr")
    headers = [td.get_text(strip=True) for td in rows[0].find_all("td")]
    records = []
    for row in rows[1:]:
        cols = row.find_all("td")
        values = [td.get_text(strip=True) for td in cols]
        pd_id = None
        for td in cols:
            a_tag = td.find("a", onclick=viewpd_pattern)
            if a_tag:
                match = viewpd_pattern.search(a_tag["onclick"])
                if match:
                    pd_id = match.group(1)
        record = dict(zip(headers, values))
        if pd_id:
            record["pd_id"] = pd_id
        records.append(record)
    return records


def extract_total_pages_and_records(html):
    soup = BeautifulSoup(html, "html.parser")
    controls_li = soup.select_one(".pagination li.disabled.controls")
    logging.debug(f"controls_li = {controls_li}")

    if controls_li:
        inputs = controls_li.find_all("input")
        if len(inputs) >= 2:
            current_page = int(inputs[0]["value"])
            total_pages = int(inputs[1]["value"])
            # ç„¶åå†æ‰¾â€œå…± NNN æ¡â€ä¸­çš„ NNNï¼š
            text = controls_li.get_text()
            match = re.search(r"å…±\s*(\d+)\s*æ¡", text)
            if match:
                total_records = int(match.group(1))
                return total_pages, total_records
    return None, None


def upsert_to_db(record, cursor):
    djzh = record.get("ç™»è®°è¯å·")
    cursor.execute("SELECT 1 FROM pesticide_data WHERE ç™»è®°è¯å· = ?", (djzh,))
    exists = cursor.fetchone() is not None

    try:
        cursor.execute(
            """
            INSERT INTO pesticide_data (
                ç™»è®°è¯å·, å†œè¯åç§°, å†œè¯ç±»åˆ«, å‰‚å‹, æ€»å«é‡, æœ‰æ•ˆæœŸè‡³, ç™»è®°è¯æŒæœ‰äºº, pd_id
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(ç™»è®°è¯å·) DO UPDATE SET
                å†œè¯åç§°=excluded.å†œè¯åç§°,
                å†œè¯ç±»åˆ«=excluded.å†œè¯ç±»åˆ«,
                å‰‚å‹=excluded.å‰‚å‹,
                æ€»å«é‡=excluded.æ€»å«é‡,
                æœ‰æ•ˆæœŸè‡³=excluded.æœ‰æ•ˆæœŸè‡³,
                ç™»è®°è¯æŒæœ‰äºº=excluded.ç™»è®°è¯æŒæœ‰äºº,
                pd_id=excluded.pd_id
        """,
            (
                djzh,
                record.get("å†œè¯åç§°"),
                record.get("å†œè¯ç±»åˆ«"),
                record.get("å‰‚å‹"),
                record.get("æ€»æœ‰æ•ˆæˆåˆ†å«é‡") or record.get("æ€»å«é‡"),
                record.get("æœ‰æ•ˆæœŸè‡³"),
                record.get("ç™»è®°è¯æŒæœ‰äºº"),
                record.get("pd_id"),
            ),
        )

        if exists:
            # logging.info(f"æ›´æ–°ç™»è®°è¯å·ï¼š{djzh}")
            pass
        else:
            logging.info(f"æ–°å¢ç™»è®°è¯å·ï¼š{djzh}")

    except Exception as e:
        logging.error(f"âŒ DB insert/update error for {djzh}: {e}")


def update_total_expected_from_page(html, total_expected):
    pages, total = extract_total_pages_and_records(html)
    logging.info(f"ğŸ“Š æ€»é¡µæ•°ï¼š{pages}, æ€»è®°å½•æ•°ï¼š{total}")
    if total and total > total_expected:
        total_expected = total
    return pages, total_expected


def should_stop_by_db_count(cursor, total_expected):
    """
    åˆ¤æ–­æ•°æ®åº“ä¸­è®°å½•æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œé€‚ç”¨äºæå‰ç»ˆæ­¢é€»è¾‘ã€‚
    """
    cursor.execute("SELECT COUNT(*) FROM pesticide_data")
    current_total = cursor.fetchone()[0]
    logging.info(f"ğŸ“Š å½“å‰å·²åŒæ­¥æ•°æ®æ€»æ•°: {current_total}")
    return current_total >= total_expected


# === ä¸»é€»è¾‘ ===
def main(total_expected=50716, end_page=2536):
    start_page = load_progress()
    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()

    logging.info(
        f"-------------{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} å¼€å§‹æŠ“å–ï¼Œé¢„è®¡æ€»é¡µæ•° {end_page}ï¼Œé¢„è®¡æ€»æ¡æ•°çº¦ {total_expected}ï¼Œä»ç¬¬ {start_page} é¡µå¼€å§‹-------------"
    )
    pbar = tqdm(total=end_page - start_page + 1, desc="è·å–åˆ—è¡¨æ•°æ®ä¸­", ncols=100)
    page = start_page

    try:
        while page <= end_page:
            html = fetch_list_page(page)
            if not html:
                logging.error(f"âŒ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥")
                page += 1
                pbar.update(1)
                continue

            # åŠ¨æ€æ›´æ–°é¡µæ•°å’Œæ€»æ¡æ•°ï¼ˆé˜²æ­¢é—æ¼ï¼‰
            pages, new_total = update_total_expected_from_page(html, total_expected)
            if new_total and new_total > total_expected:
                total_expected = new_total
                logging.info(f"ğŸ“ˆ åŠ¨æ€æ›´æ–° total_expected = {total_expected}")

            if pages and 1 < pages < end_page:
                delta = end_page - pages
                end_page = pages
                pbar.total -= delta
                pbar.refresh()
                logging.info(f"ğŸ“‰ åŠ¨æ€è°ƒæ•´æ€»é¡µæ•°ä¸º {end_page}")

            try:
                records = parse_list(html)
                for record in records:
                    if "ç™»è®°è¯å·" in record and record["ç™»è®°è¯å·"]:
                        upsert_to_db(record, cursor)
            except Exception as e:
                logging.error(f"âŒ ç¬¬ {page} é¡µè§£æ/å…¥åº“å¤±è´¥: {e}")

            # æ¯é¡µæäº¤
            conn.commit()
            save_progress(page)
            page += 1
            pbar.update(1)

            # æ¯ 5 é¡µæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦è¾¾åˆ°ç›®æ ‡æ•°æ®é‡
            if page % 5 == 0:
                cursor.execute("SELECT COUNT(*) FROM pesticide_data")
                current_total = cursor.fetchone()[0]
                logging.info(f"ğŸ“Š å½“å‰å·²åŒæ­¥æ•°æ®æ€»æ•°: {current_total}")
                if current_total >= total_expected:
                    logging.info(f"âœ… æ•°æ®å·²è¾¾ {total_expected}ï¼Œæå‰ç»“æŸçˆ¬è™«ä»»åŠ¡")
                    break

            # ä¿æ´»ï¼ˆé˜²æ­¢æ•°æ®åº“è¿æ¥è¶…æ—¶ï¼‰
            if page % 20 == 0:
                conn.commit()
                conn.close()
                conn = sqlite3.connect(DB_FILE)
                cursor = conn.cursor()

            time.sleep(random.uniform(0.5, 1.2))

    except KeyboardInterrupt:
        logging.warning("ğŸ›‘ æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ‰‹åŠ¨ç»ˆæ­¢ä»»åŠ¡ã€‚ä¿å­˜è¿›åº¦åé€€å‡º...")
        save_progress(page)

    finally:
        pbar.close()
        conn.close()
        logging.info(
            f"âœ… åŒæ­¥ä»»åŠ¡ç»“æŸï¼Œæœ€ç»ˆæŠ“å–è‡³ç¬¬ {page - 1} é¡µï¼ŒæœŸæœ›æ¡æ•° {total_expected}"
        )


def test1():

    url = "https://www.icama.cn/BasicdataSystem/pesticideRegistration/queryselect.do"
    response = requests.get(url, headers=HEADERS)

    pages, records = extract_total_pages_and_records(response.text)
    print("-" * 10)
    # ä¿å­˜ HTML åˆ°æœ¬åœ°æ–‡ä»¶
    with open(
        os.path.join(os.path.dirname(__file__), "data/page_sample.html"),
        "w",
        encoding="utf-8",
    ) as f:
        f.write(response.text)
    print(f"ğŸ“Š æ€»é¡µæ•° {pages} é¡µ, æ€»æ¡æ•°: {records}")
    print("-" * 10)


def test():
    with open(
        os.path.join(os.path.dirname(__file__), "data/page_sample.html"),
        encoding="utf-8",
    ) as f:
        html = f.read()
    pages, records = extract_total_pages_and_records(html)
    print("-" * 10)
    print(f"ğŸ“Š æ€»é¡µæ•° {pages} é¡µ, æ€»æ¡æ•°: {records}")
    print("-" * 10)


if __name__ == "__main__":
    main()
