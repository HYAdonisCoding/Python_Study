import sqlite3
import json
import os
import time
import random
import logging
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

DATA_DIR = "data"
DB_FILE = os.path.join(DATA_DIR, "pesticide_data.db")
MAX_RETRIES = 2
MAX_WORKERS = 1
LOG_FILE = os.path.join(DATA_DIR, "fix_empty_ingredients.log")

logging.basicConfig(
    filename=LOG_FILE,
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
)

logging.getLogger("WDM").setLevel(logging.WARNING)

def build_url(pd_id):
    r_value = round(random.random(), 16)
    return f"https://www.icama.cn/BasicdataSystem/pesticideRegistration/viewpd.do?r={r_value}&id={pd_id}"


def parse_detail_page(soup):
    tables = soup.find_all("table", {"id": "reg"})
    result = {"ç™»è®°è¯ä¿¡æ¯": {}, "æœ‰æ•ˆæˆåˆ†ä¿¡æ¯": [], "åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯": []}

    for table in tables:
        title_cell = table.find("td")
        if not title_cell:
            continue
        title = title_cell.get_text(strip=True)
        # print(f"âš™ï¸ è¡¨æ ¼æ ‡é¢˜è¯†åˆ«ï¼š{title}")
        # print(json.dumps(result, ensure_ascii=False, indent=2))

        if "å†œè¯ç™»è®°è¯ä¿¡æ¯" in title:
            for row in table.find_all("tr")[1:]:
                cols = [td.get_text(strip=True) for td in row.find_all("td")]
                for i in range(0, len(cols), 2):
                    key = cols[i].replace("ï¼š", "").strip()
                    value = cols[i + 1] if i + 1 < len(cols) else ""
                    if key:
                        result["ç™»è®°è¯ä¿¡æ¯"][key] = value

        elif "æœ‰æ•ˆæˆåˆ†ä¿¡æ¯" in title:
            for row in table.find_all("tr")[2:]:
                cols = [td.get_text(strip=True) for td in row.find_all("td")]
                if len(cols) == 3:
                    result["æœ‰æ•ˆæˆåˆ†ä¿¡æ¯"].append(
                        {
                            "æœ‰æ•ˆæˆåˆ†": cols[0],
                            "æœ‰æ•ˆæˆåˆ†è‹±æ–‡å": cols[1],
                            "æœ‰æ•ˆæˆåˆ†å«é‡": cols[2],
                        }
                    )

        elif "åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯" in title:
            for row in table.find_all("tr")[2:]:
                cols = [td.get_text(strip=True) for td in row.find_all("td")]
                if len(cols) >= 4:
                    result["åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯"].append(
                        {
                            "ä½œç‰©/åœºæ‰€": cols[0],
                            "é˜²æ²»å¯¹è±¡": cols[1],
                            "ç”¨è¯é‡": cols[2],
                            "æ–½ç”¨æ–¹æ³•": cols[3],
                        }
                    )

    return result


def extract_all_info_with_selenium(url, retries=MAX_RETRIES):
    for attempt in range(1, retries + 1):
        try:
            options = webdriver.ChromeOptions()
            # DOM Ready å°±è¿”å›ï¼Œä¸ç­‰æ‰€æœ‰ JS / å›¾ç‰‡ / iframe
            options.page_load_strategy = "eager"
            options.add_argument("--headless")
            options.add_argument("--disable-gpu")
            options.add_argument("--no-sandbox")
            options.add_argument("--ignore-certificate-errors")  # å¿½ç•¥è¯ä¹¦é”™è¯¯
            options.add_argument("--allow-insecure-localhost")  # å…è®¸ä¸å®‰å…¨çš„ https
            options.add_argument("--disable-web-security")  # å…³é—­éƒ¨åˆ†å®‰å…¨æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰
            driver = webdriver.Chrome(
                service=Service(ChromeDriverManager().install()), options=options
            )
            driver.set_page_load_timeout(60)
            driver.get(url)

            # ç­‰å¾…è¡¨æ ¼åŠ è½½
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, "reg"))
            )

            time.sleep(1.5)  # é¡µé¢æ¸²æŸ“æ—¶é—´

            html = driver.page_source
            soup = BeautifulSoup(html, "html.parser")
            logging.info(f"æˆåŠŸè·å–å¹¶è§£æé¡µé¢: {url}")
            return parse_detail_page(soup)

        except Exception as e:
            logging.warning(f"âš ï¸ ç¬¬ {attempt} æ¬¡è¯·æ±‚å¤±è´¥: {url} - {e}")
            time.sleep(random.uniform(2, 4))
        finally:
            try:
                driver.quit()
            except:
                pass

    logging.error(f"âŒ æœ€ç»ˆå¤±è´¥ï¼Œæ”¾å¼ƒæŠ“å–: {url}")
    return None


def process_record(record):
    djzh, pd_id = record
    url = build_url(pd_id)
    logging.info(f"ğŸŒ å¼€å§‹åŠ è½½é¡µé¢æ•°æ®: {djzh}")
    detail_info = extract_all_info_with_selenium(url)

    if detail_info and detail_info["æœ‰æ•ˆæˆåˆ†ä¿¡æ¯"]:
        logging.info(f"æ•°æ®æå–æˆåŠŸï¼Œå‡†å¤‡å†™å…¥æ•°æ®åº“: {djzh}")
        try:
            conn = sqlite3.connect(DB_FILE)
            cursor = conn.cursor()
            cursor.execute(
                """
                UPDATE pesticide_data
                SET ç™»è®°è¯ä¿¡æ¯ = ?, æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ = ?, åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯ = ?,æ›´æ–°æ—¶é—´ = DATETIME('now', 'localtime')
                WHERE ç™»è®°è¯å· = ?
            """,
                (
                    json.dumps(detail_info.get("ç™»è®°è¯ä¿¡æ¯", {}), ensure_ascii=False),
                    json.dumps(detail_info.get("æœ‰æ•ˆæˆåˆ†ä¿¡æ¯", []), ensure_ascii=False),
                    json.dumps(
                        detail_info.get("åˆ¶å‰‚ç”¨è¯é‡ä¿¡æ¯", []), ensure_ascii=False
                    ),
                    djzh,
                ),
            )
            conn.commit()
            conn.close()
            logging.info(f"âœ… Updated {djzh}")
        except Exception as e:
            logging.error(f"âŒ DB update failed for {djzh}: {e}")
    else:
        logging.info(f"è·³è¿‡ {djzh}ï¼Œæœªæå–åˆ°æœ‰æ•ˆæˆåˆ†ä¿¡æ¯")
        logging.warning(f"âš ï¸ No valid info found for {djzh} (URL: {url})")


def main():
    logging.info("çˆ¬è™«å¯åŠ¨")

    conn = sqlite3.connect(DB_FILE)
    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT ç™»è®°è¯å·, pd_id
        FROM pesticide_data
        WHERE æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ IS NULL OR æœ‰æ•ˆæˆåˆ†ä¿¡æ¯ = '[]'
        """
    )
    records = cursor.fetchall()
    conn.close()

    total = len(records)
    completed = 0
    logging.info(f"Total records to update: {total}")

    executor = ThreadPoolExecutor(max_workers=MAX_WORKERS)

    try:
        futures = [executor.submit(process_record, rec) for rec in records]

        for _ in tqdm(
            as_completed(futures),
            total=total,
            desc="è¡¥å…¨ä¸­",
            dynamic_ncols=True,
        ):
            completed += 1

    except KeyboardInterrupt:
        logging.warning(f"â›” å·²å®Œæˆ {completed}/{total}ï¼Œä»»åŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        executor.shutdown(wait=False, cancel_futures=True)
        os._exit(130)
    except Exception as e:
        logging.exception(f"âŒ ä¸»æµç¨‹å¼‚å¸¸: {e}")
        executor.shutdown(wait=False, cancel_futures=True)
        raise

    else:
        logging.info("âœ… æ‰€æœ‰è¡¥å…¨ä»»åŠ¡å®Œæˆã€‚")

    finally:
        # å…œåº•ï¼Œç¡®ä¿èµ„æºé‡Šæ”¾
        executor.shutdown(wait=False)



if __name__ == "__main__":
    main()
    # url = build_url("5602b94c384e4795b226cee1fb723a23")
    # data = extract_all_info_with_selenium(url)
    # print(json.dumps(data, ensure_ascii=False, indent=2))
